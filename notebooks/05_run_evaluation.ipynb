{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a46f5a",
   "metadata": {},
   "source": [
    "# Step 5 â€“ Run Model Evaluation\n",
    "\n",
    "This notebook runs the evaluation process for the trained multi-label image classifier.\n",
    "It loads the specified model checkpoint, processes the validation dataset, calculates the micro F1 score, and logs this metric to MLflow.\n",
    "This provides a direct way to assess model performance using the core evaluation logic from `src/evaluate.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root '/workspaces/photo_tag_pipeline' added to sys.path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.models.metrics' from '/workspaces/photo_tag_pipeline/src/models/metrics.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add the project root to the Python path\n",
    "# This allows importing modules from the 'src' directory\n",
    "current_path = Path(os.getcwd()).resolve()\n",
    "project_root = None\n",
    "# Iterate up from current_path to its parents\n",
    "for parent_dir in [current_path] + list(current_path.parents):\n",
    "    if (parent_dir / \".git\").is_dir() or (parent_dir / \"pyproject.toml\").is_file() or (parent_dir / \"src\").is_dir():\n",
    "        project_root = parent_dir\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    # Fallback for structures where notebook is in 'notebooks' dir directly under project root\n",
    "    if current_path.name == \"notebooks\" and (current_path.parent / \"src\").is_dir():\n",
    "        project_root = current_path.parent\n",
    "    else:\n",
    "        # Default to current_path if specific markers or 'notebooks' structure isn't found\n",
    "        project_root = current_path\n",
    "        print(f\"Warning: Could not reliably find project root. Using CWD: {project_root}. Ensure 'src' is in python path.\")\n",
    "\n",
    "if project_root:\n",
    "    project_root_str = str(project_root)\n",
    "    if project_root_str not in sys.path:\n",
    "        sys.path.insert(0, project_root_str)\n",
    "        print(f\"Project root '{project_root_str}' added to sys.path.\")\n",
    "    else:\n",
    "        print(f\"Project root '{project_root_str}' is already in sys.path.\")\n",
    "else:\n",
    "    print(\"Error: Project root could not be determined. Imports from 'src' may fail.\")\n",
    "\n",
    "# Reload modules to ensure the latest changes are picked up\n",
    "# Useful if you're actively developing the src modules\n",
    "import src.config\n",
    "import src.data.loader\n",
    "import src.models.model\n",
    "import src.utils.metrics\n",
    "\n",
    "importlib.reload(src.config)\n",
    "importlib.reload(src.data.loader)\n",
    "importlib.reload(src.models.model)\n",
    "importlib.reload(src.models.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "ModelConfig: ModelConfig(backbone='resnet18', pretrained=True, drop_rate=0.0, num_classes=None)\n",
      "TrainConfig (for data loading defaults): TrainConfig(epochs=5, seed=42, early_stop_patience=3, batch_size=32, num_workers=2)\n",
      "Attempting to load num_classes from metadata...\n",
      "Number of classes loaded from metadata (/workspaces/photo_tag_pipeline/src/data/coco/dataset_metadata.json): 2\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Imports from our src directory\n",
    "from src.config import ModelConfig, CHECKPOINT_DIR, META_PATH, TrainConfig # Added TrainConfig for load_data defaults\n",
    "from src.data.loader import load_data\n",
    "from src.models.model import build_model\n",
    "from src.utils.metrics import micro_f1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ---- Configuration ----\n",
    "mcfg = ModelConfig()\n",
    "# tcfg is implicitly used by load_data for batch_size, num_workers if not overridden\n",
    "# We can instantiate it if we want to explicitly pass its values or check them\n",
    "tcfg = TrainConfig() \n",
    "print(f\"ModelConfig: {mcfg}\")\n",
    "print(f\"TrainConfig (for data loading defaults): {tcfg}\")\n",
    "\n",
    "\n",
    "# ---- Ensure num_classes is set in ModelConfig ----\n",
    "if mcfg.num_classes is None:\n",
    "    print(\"Attempting to load num_classes from metadata...\")\n",
    "    try:\n",
    "        if META_PATH.exists():\n",
    "            with open(META_PATH, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            mcfg.num_classes = metadata.get('num_classes')\n",
    "            print(f\"Number of classes loaded from metadata ({META_PATH}): {mcfg.num_classes}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Metadata file not found at {META_PATH}, num_classes not set.\")\n",
    "        if mcfg.num_classes is None:\n",
    "             raise ValueError(\"num_classes is None even after trying to load from metadata.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading num_classes from metadata: {e}.\")\n",
    "        print(\"Please ensure ModelConfig.num_classes is set or metadata (dataset_metadata.json) is correct and generated by 01_dataset_eda.ipynb.\")\n",
    "        raise e\n",
    "else:\n",
    "    print(f\"Using num_classes from ModelConfig: {mcfg.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b240389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "[INFO] Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /workspaces/photo_tag_pipeline/checkpoints/best_model_notebook.pth\n",
      "Model loaded successfully and set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "# ---- Build and Load Model ----\n",
    "print(\"Building model...\")\n",
    "model = build_model(mcfg).to(DEVICE)\n",
    "\n",
    "# Specify the checkpoint file to load\n",
    "# This should match the output of the training notebook (03_train_model.ipynb)\n",
    "ckpt_name = \"best_model_notebook.pth\" \n",
    "# Or use \"best_model.pth\" if evaluating model from `python src/train.py`\n",
    "ckpt_path = CHECKPOINT_DIR / ckpt_name\n",
    "\n",
    "if not ckpt_path.exists():\n",
    "    print(f\"ERROR: Checkpoint file not found at {ckpt_path}\")\n",
    "    print(\"Please ensure you have run the training process (e.g., 03_train_model.ipynb or src/train.py) first.\")\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "else:\n",
    "    print(f\"Loading checkpoint from: {ckpt_path}\")\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully and set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c9b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n",
      "Validation data loaded. Number of batches: 1\n"
     ]
    }
   ],
   "source": [
    "# ---- Load Data ----\n",
    "# load_data will use batch_size and num_workers from TrainConfig by default\n",
    "# if not overridden here.\n",
    "print(\"Loading validation data...\")\n",
    "# We only need the validation loader for evaluation\n",
    "# The first return value is train_loader, which we can ignore with '_'\n",
    "try:\n",
    "    _, val_loader = load_data(batch_size=tcfg.batch_size, num_workers=tcfg.num_workers)\n",
    "    print(f\"Validation data loaded. Number of batches: {len(val_loader)}\")\n",
    "    if len(val_loader) == 0:\n",
    "        print(\"Warning: Validation loader is empty. Check dataset splits and paths.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Ensure that the dataset has been prepared (01_dataset_eda.ipynb) and paths in config.py are correct.\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f892afb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/1 [00:00<?, ?batch/s]/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.01s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loop completed.\n",
      "Shape of y_pred: (3, 2), Shape of y_true: (3, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Run Evaluation Loop ----\n",
    "print(\"Starting evaluation...\")\n",
    "preds_list, gts_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar_eval = tqdm(val_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "    for imgs, labels in progress_bar_eval:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        # Labels are already on CPU from dataloader, no need to move to DEVICE then back\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        # Apply sigmoid and threshold\n",
    "        # Sigmoid is applied because model outputs logits; BCEWithLogitsLoss used in training\n",
    "        # Threshold of 0.5 is common for binary relevance multi-label classification\n",
    "        probabilities = outputs.sigmoid().cpu().numpy() \n",
    "        predicted_labels = (probabilities > 0.5).astype(np.float32)\n",
    "        \n",
    "        preds_list.append(predicted_labels)\n",
    "        gts_list.append(labels.numpy()) # labels are already torch tensors on CPU\n",
    "\n",
    "# Stack predictions and ground truths\n",
    "if preds_list and gts_list:\n",
    "    y_pred = np.vstack(preds_list)\n",
    "    y_true = np.vstack(gts_list)\n",
    "    print(\"Evaluation loop completed.\")\n",
    "    print(f\"Shape of y_pred: {y_pred.shape}, Shape of y_true: {y_true.shape}\")\n",
    "else:\n",
    "    print(\"No predictions made. Validation loader might be empty or an error occurred.\")\n",
    "    y_pred, y_true = None, None # Set to None if evaluation didn't run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1280590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Micro F1 score...\n",
      "Micro F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ---- Calculate Metrics ----\n",
    "if y_pred is not None and y_true is not None:\n",
    "    print(\"Calculating Micro F1 score...\")\n",
    "    f1 = micro_f1(y_true, y_pred)\n",
    "    print(f\"Micro F1 Score: {f1:.4f}\")\n",
    "else:\n",
    "    f1 = None\n",
    "    print(\"Skipping F1 calculation as predictions are not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9dc984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run ID: 6e7e39f3c7004f209e1986bdbccf5c52 (Status: RUNNING)\n",
      "Logged f1_score_micro_notebook: 0.0000 to MLflow.\n",
      "MLflow run ended.\n",
      "\n",
      "Evaluation notebook finished.\n"
     ]
    }
   ],
   "source": [
    "# ---- Log to MLflow and Print Results ----\n",
    "if f1 is not None:\n",
    "    try:\n",
    "        # Check if an active run exists, otherwise start a new one\n",
    "        # Using a context manager for robust run management\n",
    "        with mlflow.start_run(run_name=\"evaluation_notebook\", nested=True) as run: # nested=True allows this run within a potential parent run\n",
    "            print(f\"MLflow Run ID: {run.info.run_id} (Status: {run.info.status})\")\n",
    "            mlflow.log_metric(\"f1_score_micro_notebook\", f1) # Use a distinct name\n",
    "            print(f\"Logged f1_score_micro_notebook: {f1:.4f} to MLflow.\")\n",
    "            # The run will automatically end when exiting the 'with' block\n",
    "        print(\"MLflow run ended.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MLflow logging: {e}\")\n",
    "        print(\"Ensure MLflow tracking server is configured and running if you expect remote logging.\")\n",
    "else:\n",
    "    print(\"No F1 score to log to MLflow.\")\n",
    "\n",
    "print(\"\\nEvaluation notebook finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
