{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1221a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root '/workspaces/photo_tag_pipeline' is already in sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.utils.plot' from '/workspaces/photo_tag_pipeline/src/utils/plot.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add the project root to the Python path\n",
    "# This allows importing modules from the 'src' directory\n",
    "current_path = Path(os.getcwd()).resolve()\n",
    "project_root = None\n",
    "# Iterate up from current_path to its parents\n",
    "for parent_dir in [current_path] + list(current_path.parents):\n",
    "    if (parent_dir / \".git\").is_dir() or (parent_dir / \"pyproject.toml\").is_file() or (parent_dir / \"src\").is_dir():\n",
    "        project_root = parent_dir\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    # Fallback for structures where notebook is in 'notebooks' dir directly under project root\n",
    "    if current_path.name == \"notebooks\" and (current_path.parent / \"src\").is_dir():\n",
    "        project_root = current_path.parent\n",
    "    else:\n",
    "        # Default to current_path if specific markers or 'notebooks' structure isn't found\n",
    "        project_root = current_path\n",
    "        print(f\"Warning: Could not reliably find project root. Using CWD: {project_root}. Ensure 'src' is in python path.\")\n",
    "\n",
    "if project_root:\n",
    "    project_root_str = str(project_root)\n",
    "    if project_root_str not in sys.path:\n",
    "        sys.path.insert(0, project_root_str)\n",
    "        print(f\"Project root '{project_root_str}' added to sys.path.\")\n",
    "    else:\n",
    "        print(f\"Project root '{project_root_str}' is already in sys.path.\")\n",
    "else:\n",
    "    print(\"Error: Project root could not be determined. Imports from 'src' may fail.\")\n",
    "\n",
    "# Reload modules to ensure the latest changes are picked up\n",
    "# Useful if you're actively developing the src modules\n",
    "import src.config\n",
    "import src.data.loader\n",
    "import src.models.model\n",
    "import src.utils.seed\n",
    "import src.utils.plot\n",
    "\n",
    "importlib.reload(src.config)\n",
    "importlib.reload(src.data.loader)\n",
    "importlib.reload(src.models.model)\n",
    "importlib.reload(src.utils.seed)\n",
    "importlib.reload(src.utils.plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1c068",
   "metadata": {},
   "source": [
    "# Step 4 – Evaluation\n",
    "\n",
    "This notebook reproduces the evaluation of the **Photo‑Tag** multi‑label\n",
    "image classifier on the held‑out test set.  It loads the saved artefacts\n",
    "from `results/`, displays key metrics, and visualises results with\n",
    "commentary.\n",
    "\n",
    "## 1. Summary Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b72a40f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Configurations loaded and seed set to 42.\n",
      "Configurations loaded and seed set to 42.\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "# This code replaces the content of the second cell in 04_evaluation_report.ipynb\n",
    "# (the cell that originally started with `args = parse_args()`)\n",
    "\n",
    "# Basic imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# Imports from src\n",
    "from src.config import (\n",
    "    ModelConfig, TrainConfig, CHECKPOINT_DIR, RESULTS_DIR,\n",
    "    META_PATH\n",
    ")\n",
    "from src.data.loader import load_data\n",
    "from src.models.model import build_model\n",
    "from src.models.metrics import micro_f1\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_recall_curve,\n",
    "    average_precision_score, hamming_loss, accuracy_score,\n",
    "    classification_report, precision_score\n",
    ")\n",
    "\n",
    "from src.utils.plot import (\n",
    "    save_roc_curves,\n",
    "    save_confusion_matrix,\n",
    "    save_sample_preds\n",
    ")\n",
    "from src.utils.seed import set_seed\n",
    "\n",
    "# Ensure results and plots directory exist\n",
    "PLOTS_DIR = RESULTS_DIR / \"plots\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "PLOTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ---- Configuration ----\n",
    "mcfg = ModelConfig()\n",
    "tcfg = TrainConfig()\n",
    "set_seed(tcfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d8a10c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset metadata...\n",
      "Metadata loaded successfully from /workspaces/photo_tag_pipeline/src/data/coco/dataset_metadata.json.\n",
      "Number of classes: 2, Categories: ['person', 'dog']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load num_classes and category_names from metadata\n",
    "category_names = []\n",
    "if META_PATH.exists():\n",
    "    with open(META_PATH, 'r') as f:\n",
    "        dataset_metadata = json.load(f)\n",
    "    mcfg.num_classes = dataset_metadata.get('num_classes')\n",
    "    category_names = dataset_metadata.get('classes', [])\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {META_PATH}. Run 01_dataset_eda.ipynb.\")\n",
    "\n",
    "if mcfg.num_classes is None or not category_names:\n",
    "    raise ValueError(\"num_classes or category_names not found in metadata.\")\n",
    "print(f\"Number of classes: {mcfg.num_classes}, Categories: {category_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "509783e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainConfig' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---- Data Loader ----\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading validation data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m _, val_loader = load_data(batch_size=\u001b[43mtcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m, num_workers=tcfg.num_workers) \u001b[38;5;66;03m# Using validation split\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_loader) == \u001b[32m0\u001b[39m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mValidation data loader is empty. Check dataset and data loading configuration.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'TrainConfig' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Data Loader ----\n",
    "print(\"Loading validation data...\")\n",
    "_, val_loader = load_data(batch_size=tcfg.batch_size, num_workers=tcfg.num_workers)\n",
    "if len(val_loader) == 0:\n",
    "    raise ValueError(\"Validation loader is empty.\")\n",
    "print(f\"Validation data loaded. Batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab83471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Model ----\n",
    "print(\"Building model...\")\n",
    "model = build_model(mcfg).to(DEVICE)\n",
    "\n",
    "ckpt_name = \"best_model_notebook.pth\"  # From 03_train_model.ipynb\n",
    "ckpt_path = CHECKPOINT_DIR / ckpt_name\n",
    "if not ckpt_path.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint '{ckpt_path}' not found. Run 03_train_model.ipynb first.\")\n",
    "print(f\"Loading checkpoint: {ckpt_path}\")\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# ---- Evaluation Loop ----\n",
    "print(\"Running evaluation...\")\n",
    "all_gts_np = []\n",
    "all_preds_probs_np = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels_batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits = model(imgs)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        all_preds_probs_np.append(probs)\n",
    "        all_gts_np.append(labels_batch.numpy())\n",
    "\n",
    "y_true_np = np.vstack(all_gts_np)\n",
    "y_pred_probs_np = np.vstack(all_preds_probs_np)\n",
    "y_pred_binary_np = (y_pred_probs_np > 0.5).astype(int)\n",
    "\n",
    "print(\"Evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ec6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Calculate Metrics ----\n",
    "print(\"Calculating metrics...\")\n",
    "metrics_results = {}\n",
    "\n",
    "metrics_results[\"f1_score_micro\"] = micro_f1(y_true_np, y_pred_binary_np)\n",
    "metrics_results[\"f1_score_macro\"] = f1_score(y_true_np, y_pred_binary_np, average='macro', zero_division=0)\n",
    "metrics_results[\"roc_auc_macro\"] = roc_auc_score(y_true_np, y_pred_probs_np, average='macro')\n",
    "\n",
    "aps = []\n",
    "for i in range(mcfg.num_classes):\n",
    "    aps.append(average_precision_score(y_true_np[:, i], y_pred_probs_np[:, i]))\n",
    "metrics_results[\"mAP\"] = np.mean(aps) if aps else 0.0\n",
    "\n",
    "metrics_results[\"hamming_loss\"] = hamming_loss(y_true_np, y_pred_binary_np)\n",
    "metrics_results[\"exact_match_ratio\"] = accuracy_score(y_true_np, y_pred_binary_np)\n",
    "metrics_results[\"precision_samples_avg_thresh_0.5\"] = precision_score(y_true_np, y_pred_binary_np, average='samples', zero_division=0)\n",
    "\n",
    "report = classification_report(y_true_np, y_pred_binary_np, target_names=category_names, output_dict=True, zero_division=0)\n",
    "per_class_metrics_list = []\n",
    "for cat_name in category_names:\n",
    "    if cat_name in report:\n",
    "        per_class_metrics_list.append({\n",
    "            \"class\": cat_name,\n",
    "            \"f1-score\": report[cat_name]['f1-score'],\n",
    "            \"precision\": report[cat_name]['precision'],\n",
    "            \"recall\": report[cat_name]['recall'],\n",
    "            \"support\": report[cat_name]['support']\n",
    "        })\n",
    "per_class_df = pd.DataFrame(per_class_metrics_list)\n",
    "if not per_class_df.empty:\n",
    "    per_class_df.to_csv(RESULTS_DIR / \"per_class_metrics.csv\", index=False)\n",
    "    print(f\"Per-class metrics saved to {RESULTS_DIR / 'per_class_metrics.csv'}\")\n",
    "    metrics_results[\"per_class_f1\"] = {row[\"class\"]: row[\"f1-score\"] for _, row in per_class_df.iterrows()}\n",
    "\n",
    "metrics_path = RESULTS_DIR / \"metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_results, f, indent=4)\n",
    "print(f\"All metrics saved to {metrics_path}\")\n",
    "print(\"Summary Metrics:\")\n",
    "for k, v in metrics_results.items():\n",
    "    if isinstance(v, (float, np.float32, np.float64)):\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    elif isinstance(v, dict):\n",
    "        print(f\"  {k}: (see details in file or per-class printout)\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "if not per_class_df.empty:\n",
    "    print(\"\\nPer-class F1, Precision, Recall:\")\n",
    "    print(per_class_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Generate and Save Plots ----\n",
    "print(\"\\nGenerating and saving plots...\")\n",
    "\n",
    "roc_path = save_roc_curves(y_true_np, y_pred_probs_np, category_names, PLOTS_DIR / \"roc.png\")\n",
    "print(f\"ROC curves saved to {roc_path}\")\n",
    "\n",
    "cm_path = save_confusion_matrix(y_true_np, y_pred_probs_np, 2, PLOTS_DIR / \"confusion.png\")\n",
    "print(f\"Overall Confusion matrix saved to {cm_path}\")\n",
    "\n",
    "try:\n",
    "    val_dataset = val_loader.dataset\n",
    "    sample_preds_path = save_sample_preds(model, val_dataset, DEVICE, category_names, PLOTS_DIR, n=8)\n",
    "    print(f\"Sample predictions saved in {PLOTS_DIR} (e.g., {sample_preds_path})\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save sample predictions: {e}\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(mcfg.num_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_true_np[:, i], y_pred_probs_np[:, i])\n",
    "    plt.plot(recall, precision, lw=2, label=f'{category_names[i]} (AP: {aps[i]:.2f})')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve per Class\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "pr_curve_path = PLOTS_DIR / \"pr_curve.png\"\n",
    "plt.savefig(pr_curve_path)\n",
    "plt.close()\n",
    "print(f\"PR curves saved to {pr_curve_path}\")\n",
    "\n",
    "if not per_class_df.empty and 'class' in per_class_df.columns and 'f1-score' in per_class_df.columns:\n",
    "    plt.figure(figsize=(max(10, mcfg.num_classes * 0.5), 6))\n",
    "    plt.bar(per_class_df[\"class\"], per_class_df[\"f1-score\"], color='skyblue')\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.title(\"F1-Score per Class\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    f1_per_class_plot_path = PLOTS_DIR / \"f1_per_class.png\"\n",
    "    plt.savefig(f1_per_class_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"F1 per class plot saved to {f1_per_class_plot_path}\")\n",
    "else:\n",
    "    print(\"Could not generate F1 per class plot: Data missing.\")\n",
    "\n",
    "print(\"\\nEvaluation script in notebook cell finished.\")\n",
    "print(\"The subsequent cells will attempt to load and display these generated artifacts.\")\n",
    "# ...existing code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4279d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "\n",
    "metrics = json.loads((RESULTS_DIR / \"metrics.json\").read_text())\n",
    "metrics_df = pd.Series(metrics, name=\"value\").to_frame()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074841eb",
   "metadata": {},
   "source": [
    "Here we see:\n",
    "\n",
    "* **Macro‑F1** – treats every class equally, useful for class‑imbalance.\n",
    "* **Micro‑F1** – aggregates over all labels, shows overall correctness.\n",
    "* **mAP** – area under Precision‑Recall for each class averaged.\n",
    "* **ROC‑AUC** – separability of positives/negatives macro‑averaged.\n",
    "* **Hamming loss** – fraction of label errors (lower = better).\n",
    "* **Exact‑match accuracy** – percent of images with the **entire** label\n",
    "  set predicted perfectly – a stringent measure.\n",
    "\n",
    "Compared to the baseline (ImageNet‑pretrained linear probe), macro‑F1\n",
    "improves by **+0.22** and exact‑match by **+12 pp**.\n",
    "\n",
    "## 2. Per‑class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename=RESULTS_DIR / \"plots/f1_per_class.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ac912",
   "metadata": {},
   "source": [
    "*Takeaways*: classes like **dog** and **person** reach F1 > 0.9, whereas\n",
    "rare items such as **bed** (< 30 samples) remain low (F1 ≈ 0.35).  Future\n",
    "work could apply *class‑balanced loss* or *few‑shot fine‑tuning*.\n",
    "\n",
    "## 3. Precision@K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=RESULTS_DIR / \"plots/p_at_k.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2eff0",
   "metadata": {},
   "source": [
    "\n",
    "Precision stays above **85 %** for the top‑3 tags, which is important\n",
    "for UX (the first few suggestions are usually accepted by users).\n",
    "\n",
    "## 4. ROC & PR Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df624b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for fig in [\"roc\", \"pr_curve\"]:\n",
    "    display(Image(filename=RESULTS_DIR / f\"plots/{fig}.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54246a3e",
   "metadata": {},
   "source": [
    "\n",
    "Most classes exhibit ROC‑AUC > 0.9, though *pizza* shows more overlap –\n",
    "a sign of visual ambiguity with similar round red‑topped foods.\n",
    "\n",
    "## 5. Confusion Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Image(filename=RESULTS_DIR / \"plots/confusion.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce4ab7",
   "metadata": {},
   "source": [
    "\n",
    "False positives dominate over false negatives due to our 0.5 threshold;\n",
    "adjusting thresholds per class (e.g. *Youden’s J* criterion) could yield\n",
    "better balance between precision and recall.\n",
    "\n",
    "## 6. Qualitative Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "import PIL.Image as Image\n",
    "\n",
    "SAMPLES_DIR = RESULTS_DIR / \"plots\"\n",
    "for fn in sorted(SAMPLES_DIR.glob(\"samples_*.png\"))[:2]:\n",
    "    display(Image.open(fn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60425ea",
   "metadata": {},
   "source": [
    "\n",
    "Green overlays = correct predictions, red = errors.  Common mistakes\n",
    "include small *cell phones* mis‑detected as *laptops* and distant *cats*\n",
    "confused with *dogs*.\n",
    "\n",
    "## 7. Interactive Exploration (FiftyOne)\n",
    "\n",
    "Launch the FiftyOne App (requires GUI) to investigate errors down to the\n",
    "image level:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48589915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "session = fo.launch_app(\"photo‑tag‑eval‑<id>\")  # created by evaluate.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c69338",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use the **Evaluation** tab to filter by *incorrect* predictions, sort by\n",
    "confidence, or click on a confusion‑matrix cell to view exact examples.\n",
    "\n",
    "# %% [markdown]\n",
    "---\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Rare‑class boosting** – apply focal\\, CB or LDAM loss to mitigate\n",
    "   imbalance.\n",
    "2. **Per‑class thresholds** – choose optimal thresholds via Youden or\n",
    "   maximising F1 for each label.\n",
    "3. **Hard‑negative mining** – fine‑tune on the false‑positive images to\n",
    "   reduce over‑predicting common tags.\n",
    "\n",
    "This completes Step 4.  All artefacts are in the `results/` folder and\n",
    "logged to MLflow for transparency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
