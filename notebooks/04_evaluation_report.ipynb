{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1221a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root '/workspaces/photo_tag_pipeline' is already in sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.utils.plot' from '/workspaces/photo_tag_pipeline/src/utils/plot.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add the project root to the Python path\n",
    "# This allows importing modules from the 'src' directory\n",
    "current_path = Path(os.getcwd()).resolve()\n",
    "project_root = None\n",
    "# Iterate up from current_path to its parents\n",
    "for parent_dir in [current_path] + list(current_path.parents):\n",
    "    if (parent_dir / \".git\").is_dir() or (parent_dir / \"pyproject.toml\").is_file() or (parent_dir / \"src\").is_dir():\n",
    "        project_root = parent_dir\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    # Fallback for structures where notebook is in 'notebooks' dir directly under project root\n",
    "    if current_path.name == \"notebooks\" and (current_path.parent / \"src\").is_dir():\n",
    "        project_root = current_path.parent\n",
    "    else:\n",
    "        # Default to current_path if specific markers or 'notebooks' structure isn't found\n",
    "        project_root = current_path\n",
    "        print(f\"Warning: Could not reliably find project root. Using CWD: {project_root}. Ensure 'src' is in python path.\")\n",
    "\n",
    "if project_root:\n",
    "    project_root_str = str(project_root)\n",
    "    if project_root_str not in sys.path:\n",
    "        sys.path.insert(0, project_root_str)\n",
    "        print(f\"Project root '{project_root_str}' added to sys.path.\")\n",
    "    else:\n",
    "        print(f\"Project root '{project_root_str}' is already in sys.path.\")\n",
    "else:\n",
    "    print(\"Error: Project root could not be determined. Imports from 'src' may fail.\")\n",
    "\n",
    "# Reload modules to ensure the latest changes are picked up\n",
    "# Useful if you're actively developing the src modules and want to see changes without restarting the kernel.\n",
    "import src.config\n",
    "import src.data.loader\n",
    "import src.models.model\n",
    "import src.utils.seed\n",
    "import src.utils.plot\n",
    "\n",
    "importlib.reload(src.config)\n",
    "importlib.reload(src.data.loader)\n",
    "importlib.reload(src.models.model)\n",
    "importlib.reload(src.utils.seed)\n",
    "importlib.reload(src.utils.plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1c068",
   "metadata": {},
   "source": [
    "# Step 4 – Evaluation & Interpretation\n",
    "\n",
    "This notebook presents a comprehensive evaluation of the **Photo‑Tag** multi‑label image classifier. It focuses on assessing the model's performance on a held‑out validation/test set by:\n",
    "1. Loading the latest trained model checkpoint from the `checkpoints/` directory.\n",
    "2. Preparing the data loader for the validation set.\n",
    "3. Running the model to obtain predictions.\n",
    "4. Calculating a suite of relevant performance metrics (e.g., F1-scores, mAP, ROC-AUC).\n",
    "5. Visualizing these metrics, including per-class performance, ROC curves, precision-recall curves, and confusion matrices.\n",
    "6. Displaying qualitative examples of model predictions to offer visual insights.\n",
    "\n",
    "The goal is to provide a clear and thorough understanding of the model's strengths, weaknesses, and areas for potential improvement. All generated artifacts, such as metrics files and plots, are saved to the `results/` directory for persistence and further analysis.\n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a40f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Configurations loaded and seed set to 42.\n",
      "Configurations loaded and seed set to 42.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Imports from src\n",
    "from src.config import (ModelConfig, TrainConfig, CHECKPOINT_DIR, RESULTS_DIR, META_PATH)\n",
    "from src.data.loader import load_data\n",
    "from src.utils.metrics import micro_f1\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_recall_curve,\n",
    "    average_precision_score, hamming_loss, accuracy_score,\n",
    "    classification_report, precision_score\n",
    ")\n",
    "\n",
    "from src.utils.plot import (\n",
    "    save_roc_curves,\n",
    "    save_confusion_matrix,\n",
    "    save_sample_preds\n",
    ")\n",
    "from src.utils.seed import set_seed\n",
    "\n",
    "# Ensure results and plots directories exist for storing outputs\n",
    "PLOTS_DIR = RESULTS_DIR / \"plots\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True) \n",
    "PLOTS_DIR.mkdir(exist_ok=True, parents=True)   \n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ---- Configuration ----\n",
    "# Load model and training configurations\n",
    "mcfg = ModelConfig()\n",
    "tcfg = TrainConfig()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(tcfg.seed)\n",
    "print(f\"Configurations loaded and seed set to {tcfg.seed}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d8a10c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset metadata...\n",
      "Metadata loaded successfully from /workspaces/photo_tag_pipeline/src/data/coco/dataset_metadata.json.\n",
      "Number of classes: 2, Categories: ['person', 'dog']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load num_classes and category_names from metadata\n",
    "print(\"Loading dataset metadata...\")\n",
    "category_names = []\n",
    "if META_PATH.exists():\n",
    "    with open(META_PATH, 'r') as f:\n",
    "        dataset_metadata = json.load(f)\n",
    "    mcfg.num_classes = dataset_metadata.get('num_classes')\n",
    "    category_names = dataset_metadata.get('classes', [])\n",
    "    print(f\"Metadata loaded successfully from {META_PATH}.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {META_PATH}. Please run 01_dataset_eda.ipynb first to generate it.\")\n",
    "\n",
    "if mcfg.num_classes is None or not category_names:\n",
    "    raise ValueError(\"Number of classes (num_classes) or category names (classes) not found in metadata. Ensure metadata is correctly formatted.\")\n",
    "print(f\"Number of classes: {mcfg.num_classes}, Categories: {category_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "509783e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainConfig' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---- Data Loader ----\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading validation data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m _, val_loader = load_data(batch_size=\u001b[43mtcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m, num_workers=tcfg.num_workers) \u001b[38;5;66;03m# Using validation split\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_loader) == \u001b[32m0\u001b[39m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mValidation data loader is empty. Check dataset and data loading configuration.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'TrainConfig' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Data Loader ----\n",
    "print(\"Loading validation data...\")\n",
    "_, val_loader = load_data(batch_size=tcfg.batch_size, num_workers=tcfg.num_workers) # Using validation split\n",
    "if len(val_loader) == 0:\n",
    "    raise ValueError(\"Validation data loader is empty. Check dataset and data loading configuration.\")\n",
    "print(f\"Validation data loaded. Number of batches: {len(val_loader)}, Batch size: {tcfg.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab83471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Model ----\n",
    "print(\"Building model architecture...\")\n",
    "model = build_model(mcfg).to(DEVICE)\n",
    "print(f\"Model '{mcfg.backbone}' built and moved to {DEVICE}.\")\n",
    "\n",
    "# Find the latest checkpoint file in CHECKPOINT_DIR\n",
    "checkpoint_files = list(CHECKPOINT_DIR.glob('*.pt')) + list(CHECKPOINT_DIR.glob('*.pth'))\n",
    "if not checkpoint_files:\n",
    "    raise FileNotFoundError(f\"No checkpoint files (.pt or .pth) found in '{CHECKPOINT_DIR}'. Ensure a model has been trained and saved (e.g., by running 03_train_model.ipynb).\")\n",
    "\n",
    "# Sort by modification time to get the newest file\n",
    "latest_ckpt_path = max(checkpoint_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "print(f\"Loading latest model checkpoint: {latest_ckpt_path}\")\n",
    "model.load_state_dict(torch.load(latest_ckpt_path, map_location=DEVICE))\n",
    "model.eval() # Set model to evaluation mode\n",
    "print(\"Model checkpoint loaded successfully and set to evaluation mode.\")\n",
    "\n",
    "# ---- Evaluation Loop ----\n",
    "print(\"\\nStarting evaluation loop on the validation set...\")\n",
    "all_gts_np = []          # To store all ground truth labels\n",
    "all_preds_probs_np = []  # To store all predicted probabilities\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculations for efficiency\n",
    "    for imgs, labels_batch in tqdm(val_loader, desc=\"Evaluating model\"):\n",
    "        imgs = imgs.to(DEVICE) # Move images to the configured device\n",
    "        logits = model(imgs)   # Get model raw output (logits)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy() # Apply sigmoid for multi-label probabilities and move to CPU\n",
    "\n",
    "        all_preds_probs_np.append(probs)\n",
    "        all_gts_np.append(labels_batch.numpy()) # Store ground truth labels\n",
    "\n",
    "# Concatenate results from all batches\n",
    "y_true_np = np.vstack(all_gts_np)\n",
    "y_pred_probs_np = np.vstack(all_preds_probs_np)\n",
    "# Convert probabilities to binary predictions using a 0.5 threshold\n",
    "y_pred_binary_np = (y_pred_probs_np > 0.5).astype(int)\n",
    "\n",
    "print(\"Evaluation loop completed.\")\n",
    "print(f\"Processed {y_true_np.shape[0]} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ec6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Calculate Metrics ----\n",
    "print(\"\\nCalculating performance metrics...\")\n",
    "metrics_results = {}\n",
    "\n",
    "# Overall metrics\n",
    "metrics_results[\"f1_score_micro\"] = micro_f1(y_true_np, y_pred_binary_np)\n",
    "metrics_results[\"f1_score_macro\"] = f1_score(y_true_np, y_pred_binary_np, average='macro', zero_division=0)\n",
    "metrics_results[\"roc_auc_macro\"] = roc_auc_score(y_true_np, y_pred_probs_np, average='macro') # Use probabilities for ROC AUC\n",
    "\n",
    "# Mean Average Precision (mAP)\n",
    "aps = []\n",
    "for i in range(mcfg.num_classes):\n",
    "    aps.append(average_precision_score(y_true_np[:, i], y_pred_probs_np[:, i])) # Use probabilities for AP\n",
    "metrics_results[\"mAP\"] = np.mean(aps) if aps else 0.0\n",
    "\n",
    "metrics_results[\"hamming_loss\"] = hamming_loss(y_true_np, y_pred_binary_np)\n",
    "metrics_results[\"exact_match_ratio\"] = accuracy_score(y_true_np, y_pred_binary_np) # Subset accuracy\n",
    "metrics_results[\"precision_samples_avg_thresh_0.5\"] = precision_score(y_true_np, y_pred_binary_np, average='samples', zero_division=0)\n",
    "\n",
    "# Per-class metrics from classification_report\n",
    "report = classification_report(y_true_np, y_pred_binary_np, target_names=category_names, output_dict=True, zero_division=0)\n",
    "per_class_metrics_list = []\n",
    "for cat_name in category_names:\n",
    "    if cat_name in report: # Check if class is in report (handles cases with no true/pred samples for a class)\n",
    "        per_class_metrics_list.append({\n",
    "            \"class\": cat_name,\n",
    "            \"f1-score\": report[cat_name]['f1-score'],\n",
    "            \"precision\": report[cat_name]['precision'],\n",
    "            \"recall\": report[cat_name]['recall'],\n",
    "            \"support\": report[cat_name]['support']\n",
    "        })\n",
    "per_class_df = pd.DataFrame(per_class_metrics_list)\n",
    "\n",
    "if not per_class_df.empty:\n",
    "    per_class_csv_path = RESULTS_DIR / \"per_class_metrics.csv\"\n",
    "    per_class_df.to_csv(per_class_csv_path, index=False)\n",
    "    print(f\"Per-class metrics saved to {per_class_csv_path}\")\n",
    "    # Store per-class F1 scores in the main metrics dictionary for easy access\n",
    "    metrics_results[\"per_class_f1\"] = {row[\"class\"]: row[\"f1-score\"] for _, row in per_class_df.iterrows()}\n",
    "else:\n",
    "    print(\"Per-class metrics DataFrame is empty. No per-class metrics will be saved or added to summary.\")\n",
    "\n",
    "# Save all calculated metrics to a JSON file\n",
    "metrics_path = RESULTS_DIR / \"metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_results, f, indent=4)\n",
    "print(f\"All summary metrics saved to {metrics_path}\")\n",
    "\n",
    "print(\"\\n--- Summary Metrics ---\")\n",
    "for k, v in metrics_results.items():\n",
    "    if isinstance(v, (float, np.float32, np.float64)): # Check if value is a float type\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    elif isinstance(v, dict) and k == \"per_class_f1\":\n",
    "        print(f\"  {k}: (see per-class table below or '{per_class_csv_path}')\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "if not per_class_df.empty:\n",
    "    display(Markdown(\"### Detailed Per-Class Metrics\"))\n",
    "    print(per_class_df.to_string(index=False))\n",
    "print(\"\\nMetrics calculation finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e77a9",
   "metadata": {},
   "source": [
    "## 2. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcfdf0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating and saving plots...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_true_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating and saving plots...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ROC Curves\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m roc_path = save_roc_curves(\u001b[43my_true_np\u001b[49m, y_pred_probs_np, category_names, PLOTS_DIR / \u001b[33m\"\u001b[39m\u001b[33mroc.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mROC curves plot saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroc_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Confusion Matrix\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Note: For multi-label, a single confusion matrix might be an aggregation or for a specific class.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# The `save_confusion_matrix` function's behavior determines this.\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'y_true_np' is not defined"
     ]
    }
   ],
   "source": [
    "# ---- Generate and Save Plots ----\n",
    "# This cell generates and saves various plots to the PLOTS_DIR.\n",
    "# These visualizations help in understanding model performance graphically.\n",
    "\n",
    "print(\"\\nGenerating and saving plots...\")\n",
    "\n",
    "# ROC Curves\n",
    "roc_path = save_roc_curves(y_true_np, y_pred_probs_np, category_names, PLOTS_DIR / \"roc.png\")\n",
    "print(f\"ROC curves plot saved to: {roc_path}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "# Note: For multi-label, a single confusion matrix might be an aggregation or for a specific class.\n",
    "# The `save_confusion_matrix` function's behavior determines this.\n",
    "cm_path = save_confusion_matrix(y_true_np, y_pred_binary_np, mcfg.num_classes, PLOTS_DIR / \"confusion.png\") # Use y_pred_binary for CM\n",
    "print(f\"Confusion matrix plot saved to: {cm_path}\")\n",
    "\n",
    "# Sample Predictions\n",
    "try:\n",
    "    val_dataset = val_loader.dataset # Access the underlying dataset\n",
    "    sample_preds_path = save_sample_preds(model, val_dataset, DEVICE, category_names, PLOTS_DIR, n=8)\n",
    "    print(f\"Sample prediction images saved in '{PLOTS_DIR}' (example: {sample_preds_path})\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save sample predictions: {e}\")\n",
    "    print(\"This might be due to issues accessing dataset samples or with the plotting function itself.\")\n",
    "\n",
    "# Precision-Recall Curves per Class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(mcfg.num_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_true_np[:, i], y_pred_probs_np[:, i])\n",
    "    plt.plot(recall, precision, lw=2, label=f'{category_names[i]} (AP: {aps[i]:.2f})') # aps calculated in the previous cell\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve per Class\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "pr_curve_path = PLOTS_DIR / \"pr_curve.png\"\n",
    "plt.savefig(pr_curve_path)\n",
    "plt.close() # Close the plot to free memory\n",
    "print(f\"Precision-Recall curves plot saved to: {pr_curve_path}\")\n",
    "\n",
    "# F1-Score per Class Bar Plot\n",
    "if not per_class_df.empty and 'class' in per_class_df.columns and 'f1-score' in per_class_df.columns:\n",
    "    plt.figure(figsize=(max(10, mcfg.num_classes * 0.6), 7)) # Adjusted figure size\n",
    "    plt.bar(per_class_df[\"class\"], per_class_df[\"f1-score\"], color='skyblue')\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.title(\"F1-Score per Class\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1)) # Set y-axis ticks for better readability\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    f1_per_class_plot_path = PLOTS_DIR / \"f1_per_class.png\"\n",
    "    plt.savefig(f1_per_class_plot_path)\n",
    "    plt.close() # Close the plot\n",
    "    print(f\"F1-score per class bar plot saved to: {f1_per_class_plot_path}\")\n",
    "else:\n",
    "    print(\"Could not generate F1 per class plot: Per-class DataFrame is empty or missing required columns.\")\n",
    "\n",
    "# Note: The Precision@K plot (p_at_k.png) is typically displayed later.\n",
    "# If its generation logic belongs here (e.g., from src.utils.plot), it should be called here.\n",
    "# For example: save_precision_at_k_plot(y_true_np, y_pred_probs_np, PLOTS_DIR / \"p_at_k.png\")\n",
    "# Currently, it's assumed to be generated by a separate process or an earlier notebook if not part of src.utils.plot.\n",
    "\n",
    "print(\"\\nPlot generation finished.\")\n",
    "print(\"The subsequent cells will load and display these generated artifacts for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4279d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display as ipy_display # Renamed to avoid conflict with other display uses\n",
    "\n",
    "# Define RESULTS_DIR and PLOTS_DIR again in case this cell is run independently or out of order.\n",
    "# However, it's best practice to run notebook cells sequentially.\n",
    "RESULTS_DIR = Path(\"results\") \n",
    "PLOTS_DIR = RESULTS_DIR / \"plots\"\n",
    "\n",
    "# Load and display overall metrics from the saved JSON file\n",
    "metrics_path = RESULTS_DIR / \"metrics.json\"\n",
    "ipy_display(Markdown(\"## 3. Performance Metrics Summary\"))\n",
    "if metrics_path.exists():\n",
    "    metrics = json.loads(metrics_path.read_text())\n",
    "    # Convert to DataFrame for better display, handling nested dicts (like per_class_f1) as is.\n",
    "    metrics_display_list = []\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, dict):\n",
    "            metrics_display_list.append({\"Metric\": k, \"Value\": \"(see details elsewhere or in JSON)\"})\n",
    "        else:\n",
    "            metrics_display_list.append({\"Metric\": k, \"Value\": f\"{v:.4f}\" if isinstance(v, float) else v})\n",
    "    \n",
    "    metrics_df_display = pd.DataFrame(metrics_display_list)\n",
    "    ipy_display(metrics_df_display)\n",
    "    \n",
    "    # Also display the per_class_metrics.csv if it exists, as it's more detailed\n",
    "    per_class_csv_path = RESULTS_DIR / \"per_class_metrics.csv\"\n",
    "    if per_class_csv_path.exists():\n",
    "        ipy_display(Markdown(\"### Detailed Per-Class Metrics (from CSV)\"))\n",
    "        per_class_df_loaded = pd.read_csv(per_class_csv_path)\n",
    "        ipy_display(per_class_df_loaded)\n",
    "    else:\n",
    "        ipy_display(Markdown(\"*Per-class metrics CSV file not found.*\"))\n",
    "else:\n",
    "    ipy_display(Markdown(f\"*Metrics file not found: {metrics_path}. Please run the metric calculation cells first.*\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074841eb",
   "metadata": {},
   "source": [
    "The table(s) above summarize key multi-label classification metrics:\n",
    "\n",
    "* **Macro‑F1 Score**: The unweighted mean of the F1-scores for each class. It treats all classes equally, which is useful when class imbalance is present.\n",
    "* **Micro‑F1 Score**: Calculated globally by counting the total true positives, false negatives, and false positives across all classes. It reflects overall aggregate performance.\n",
    "* **mAP (mean Average Precision)**: The mean of Average Precision (AP) scores for each class. AP summarizes the precision-recall curve for a class into a single value and is a standard metric for ranking quality.\n",
    "* **ROC‑AUC (Macro)**: The macro-averaged Area Under the Receiver Operating Characteristic Curve. It measures the model's ability to distinguish between positive and negative classes, averaged across all classes.\n",
    "* **Hamming Loss**: The fraction of labels that are incorrectly predicted (lower is better). It penalizes each incorrect label prediction individually.\n",
    "* **Exact‑Match Ratio (Subset Accuracy)**: The percentage of samples where *all* predicted labels perfectly match the true labels. This is a very stringent metric for multi-label tasks.\n",
    "* **Precision (Samples Average, Threshold 0.5)**: The average per-sample precision, calculated based on a 0.5 decision threshold.\n",
    "\n",
    "*(Note: The statement \"Compared to the baseline (ImageNet‑pretrained linear probe), macro‑F1 improves by **+0.22** and exact‑match by **+12 pp**.\" is illustrative and refers to a specific baseline comparison not explicitly calculated in this notebook. Actual improvements depend on the specific baseline model and data used for comparison.)*\n",
    "\n",
    "## 4. Visual Analysis of Performance\n",
    "\n",
    "This section displays the plots generated earlier, providing visual insights into the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display as ipy_display, Markdown # Renamed to avoid conflict\n",
    "# PLOTS_DIR should be defined from previous cells.\n",
    "# from pathlib import Path\n",
    "# PLOTS_DIR = Path(\"results\") / \"plots\"\n",
    "\n",
    "f1_plot_path = PLOTS_DIR / \"f1_per_class.png\"\n",
    "ipy_display(Markdown(\"### F1-Score Breakdown by Class\"))\n",
    "if f1_plot_path.exists():\n",
    "    ipy_display(Image(filename=f1_plot_path))\n",
    "else:\n",
    "    ipy_display(Markdown(f\"*Plot not found: {f1_plot_path}. Ensure the plotting cell was run successfully.*\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ac912",
   "metadata": {},
   "source": [
    "The bar chart above illustrates the F1-score achieved for each individual class. This, along with the detailed per-class metrics table shown earlier (from `per_class_metrics.csv`), helps identify which classes the model performs well on and which ones it struggles with.\n",
    "\n",
    "*Takeaways*: Classes with high F1-scores (e.g., > 0.8-0.9) are well-classified. Conversely, classes with low F1-scores (e.g., < 0.5) indicate areas for improvement. For instance, rare classes (those with low \"support\" in the per-class table) often have lower F1-scores. Potential strategies for these include targeted data augmentation, using class-balanced loss functions, or exploring few-shot learning techniques.\n",
    "\n",
    "## 5. Precision@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display as ipy_display, Markdown # Renamed to avoid conflict\n",
    "# from pathlib import Path\n",
    "# PLOTS_DIR = Path(\"results\") / \"plots\" # Ensure PLOTS_DIR is defined\n",
    "\n",
    "pk_plot_path = PLOTS_DIR / \"p_at_k.png\"\n",
    "ipy_display(Markdown(\"### Precision at Top K Predictions (Precision@K)\"))\n",
    "if pk_plot_path.exists():\n",
    "    ipy_display(Image(filename=pk_plot_path))\n",
    "else:\n",
    "    ipy_display(Markdown(f\"*Plot not found: {pk_plot_path}. This plot shows the precision when considering only the top K predicted tags for each image. Ensure it was generated by the main evaluation script or a preceding notebook step if not part of `src.utils.plot`.*\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2eff0",
   "metadata": {},
   "source": [
    "Precision@K measures the proportion of correct predictions among the top K tags suggested by the model for an image. For instance, Precision@3 considers the top 3 predicted tags.\n",
    "\n",
    "A high Precision@K (e.g., Precision consistently above **85%** for the top‑3 tags) is crucial for user experience in applications where the model provides tag suggestions. It indicates that the most prominent suggestions are usually accurate and useful.\n",
    "\n",
    "## 6. ROC & PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df624b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display as ipy_display, Markdown # Renamed to avoid conflict\n",
    "# from pathlib import Path\n",
    "# PLOTS_DIR = Path(\"results\") / \"plots\" # Ensure PLOTS_DIR is defined\n",
    "\n",
    "ipy_display(Markdown(\"### Receiver Operating Characteristic (ROC) Curves per Class\"))\n",
    "roc_img_path = PLOTS_DIR / \"roc.png\"\n",
    "if roc_img_path.exists():\n",
    "    ipy_display(Image(filename=roc_img_path))\n",
    "else:\n",
    "    ipy_display(Markdown(f\"*Plot not found: {roc_img_path}. Ensure the plotting cell was run successfully.*\"))\n",
    "\n",
    "ipy_display(Markdown(\"### Precision-Recall (PR) Curves per Class\"))\n",
    "pr_img_path = PLOTS_DIR / \"pr_curve.png\"\n",
    "if pr_img_path.exists():\n",
    "    ipy_display(Image(filename=pr_img_path))\n",
    "else:\n",
    "    ipy_display(Markdown(f\"*Plot not found: {pr_img_path}. Ensure the plotting cell was run successfully.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54246a3e",
   "metadata": {},
   "source": [
    "**ROC Curves** illustrate the diagnostic ability of the classifier as its discrimination threshold is varied. Each curve plots the True Positive Rate (TPR, or Recall) against the False Positive Rate (FPR). An ideal curve hugs the top-left corner (AUC = 1), indicating high TPR and low FPR across thresholds.\n",
    "\n",
    "**Precision-Recall (PR) Curves** show the tradeoff between precision and recall for different thresholds. This is particularly informative for imbalanced datasets where a high number of true negatives might inflate ROC AUC scores. A good PR curve stays close to the top-right corner (high precision and high recall). The Area Under the PR Curve (Average Precision, AP) is a summary measure.\n",
    "\n",
    "*Observations*: Classes with ROC AUC and AP values close to 1 (e.g., >0.9) are generally well-distinguished by the model. Lower scores for certain classes might indicate visual ambiguity with other classes or insufficient/noisy training data for those specific categories.\n",
    "\n",
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display as ipy_display, Markdown # Renamed to avoid conflict\n",
    "# from pathlib import Path\n",
    "# PLOTS_DIR = Path(\"results\") / \"plots\" # Ensure PLOTS_DIR is defined\n",
    "\n",
    "cm_plot_path = PLOTS_DIR / \"confusion.png\"\n",
    "ipy_display(Markdown(\"### Overall Confusion Matrix Visualization\"))\n",
    "if cm_plot_path.exists():\n",
    "    ipy_display(Image(filename=cm_plot_path))\n",
    "else:\n",
    "    ipy_display(Markdown(f\"*Plot not found: {cm_plot_path}. Ensure the plotting cell was run successfully. The nature of this plot (e.g., aggregated, per-class example) depends on the `save_confusion_matrix` implementation in `src.utils.plot`.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce4ab7",
   "metadata": {},
   "source": [
    "The confusion matrix provides a detailed breakdown of correct and incorrect classifications. For multi-label problems, this can be represented in various ways (e.g., one matrix per class, or an aggregated view if applicable).\n",
    "\n",
    "*Interpretation*: Analyzing the confusion matrix can reveal specific misclassification patterns. For example, if one class is frequently confused with another, it points to potential visual similarity or feature overlap that the model struggles with. If false positives consistently dominate over false negatives (or vice-versa) for many classes, it might be due to the chosen prediction threshold (e.g., 0.5). Adjusting thresholds per class (e.g., using *Youden’s J statistic* or optimizing for F1-score on a validation set) could yield a better balance.\n",
    "\n",
    "## 8. Qualitative Examples: Predictions vs. Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display as ipy_display, Markdown # Renamed to avoid conflict\n",
    "import PIL.Image\n",
    "# from pathlib import Path\n",
    "# PLOTS_DIR = Path(\"results\") / \"plots\" # Ensure PLOTS_DIR is defined\n",
    "\n",
    "ipy_display(Markdown(\"### Sample Predictions vs. Ground Truth Images\"))\n",
    "# The glob pattern might need adjustment based on actual filenames from save_sample_preds\n",
    "sample_files = sorted(PLOTS_DIR.glob(\"sample_pred_*.png\")) # Try a more specific pattern first\n",
    "if not sample_files:\n",
    "    sample_files = sorted(PLOTS_DIR.glob(\"samples_*.png\")) # Fallback to a common pattern\n",
    "if not sample_files:\n",
    "    sample_files = sorted(PLOTS_DIR.glob(\"sample_*.png\")) # Broader fallback\n",
    "\n",
    "\n",
    "if sample_files:\n",
    "    ipy_display(Markdown(f\"Displaying up to 2 sample prediction images from '{PLOTS_DIR}':\"))\n",
    "    for fn in sample_files[:2]: # Display first two found sample images\n",
    "        ipy_display(PIL.Image.open(fn))\n",
    "else:\n",
    "    ipy_display(Markdown(f\"*No sample prediction images found in '{PLOTS_DIR}' matching common patterns like 'sample_pred_*.png' or 'samples_*.png'. Ensure `save_sample_preds` in the main script correctly saves images to the plots directory with a recognizable naming convention.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af49539",
   "metadata": {},
   "source": [
    "Visualizing model predictions on sample images provides intuitive insights into its behavior. These images typically show the input image, its ground truth labels, and the model's predicted labels (often with confidence scores). Correct predictions might be highlighted (e.g., in green), and errors in red.\n",
    "\n",
    "*Common Error Patterns*: Observing these examples can reveal common mistakes, such as:\n",
    "*   Misidentifying small or occluded objects (e.g., a *cell phone* partially hidden).\n",
    "*   Confusing visually similar classes (e.g., distant *cats* with *dogs*).\n",
    "*   Failing to detect objects in cluttered scenes.\n",
    "Such patterns can guide further model refinement, data augmentation strategies, or data collection efforts for problematic cases.\n",
    "\n",
    "## 9. Interactive Exploration with FiftyOne (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df146c",
   "metadata": {},
   "source": [
    "For a deeper, interactive dive into model predictions and errors at the image level, the [FiftyOne App](https://voxel51.com/docs/fiftyone/) is a powerful open-source tool. It allows for rich visualization, querying, and analysis of image and video datasets, along with model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code demonstrates how you might launch FiftyOne if you have a dataset prepared.\n",
    "# This usually involves loading your dataset (images, ground truth labels) and model predictions \n",
    "# into a FiftyOne Dataset object. \n",
    "# The dataset name \"photo-tag-eval-<id>\" mentioned in some contexts suggests it might be \n",
    "# created by a separate `evaluate.py` script.\n",
    "\n",
    "# import fiftyone as fo\n",
    "# import fiftyone.zoo as foz # For loading example datasets or models\n",
    "\n",
    "# # Example: Define a name for your FiftyOne dataset\n",
    "# dataset_name = \"photo-tag-evaluation-run\" \n",
    "\n",
    "# try:\n",
    "#     # --- Option 1: Load an existing FiftyOne dataset ---\n",
    "#     # This would be used if your main evaluation script (e.g., evaluate.py) already created and saved a FiftyOne dataset.\n",
    "#     # existing_dataset_name = \"photo-tag-eval-from-script\" # Replace with the actual name\n",
    "#     # if fo.dataset_exists(existing_dataset_name):\n",
    "#     #     dataset = fo.load_dataset(existing_dataset_name)\n",
    "#     #     print(f\"Loaded existing FiftyOne dataset: '{dataset.name}' with {len(dataset)} samples.\")\n",
    "#     # else:\n",
    "#     #     print(f\"FiftyOne dataset '{existing_dataset_name}' not found.\")\n",
    "#     #     dataset = None\n",
    "\n",
    "#     # --- Option 2: Create a new FiftyOne dataset from your evaluation results ---\n",
    "#     # This is more typical if this notebook is the primary place for detailed result analysis.\n",
    "#     # You would need 'image_paths', 'ground_truth_labels_fiftyone_format', 'predicted_labels_fiftyone_format'.\n",
    "#     \n",
    "#     # if dataset is None and 'val_loader' in locals() and y_true_np is not None and y_pred_probs_np is not None:\n",
    "#     #     print(f\"Attempting to create a new FiftyOne dataset: '{dataset_name}'\")\n",
    "#     #     dataset = fo.Dataset(dataset_name, overwrite=True) # overwrite=True to replace if it exists\n",
    "#     #     \n",
    "#     #     # Assuming val_loader.dataset.samples gives a list of (image_path, label_tensor)\n",
    "#     #     # This part needs to be adapted based on how your val_loader.dataset is structured\n",
    "#     #     samples_to_add = []\n",
    "#     #     for i, (img_path, _) in enumerate(val_loader.dataset.samples): # Example structure\n",
    "#     #         gt_labels_sample = y_true_np[i]\n",
    "#     #         pred_probs_sample = y_pred_probs_np[i]\n",
    "#     #         \n",
    "#     #         detections_gt = []\n",
    "#     #         detections_pred = []\n",
    "#     #         \n",
    "#     #         for class_idx, class_name in enumerate(category_names):\n",
    "#     #             if gt_labels_sample[class_idx] == 1:\n",
    "#     #                 detections_gt.append(fo.Detection(label=class_name, bounding_box=[0,0,1,1])) # Dummy bbox\n",
    "#     #             \n",
    "#     #             if pred_probs_sample[class_idx] > 0.5: # Using 0.5 threshold for this example\n",
    "#     #                 detections_pred.append(\n",
    "#     #                     fo.Detection(label=class_name, bounding_box=[0,0,1,1], confidence=pred_probs_sample[class_idx]) # Dummy bbox\n",
    "#     #                 )\n",
    "#     #                 \n",
    "#     #         samples_to_add.append(fo.Sample(\n",
    "#     #             filepath=str(img_path), \n",
    "#     #             ground_truth=fo.Detections(detections=detections_gt),\n",
    "#     #             predictions=fo.Detections(detections=detections_pred)\n",
    "#     #         ))\n",
    "#     #     \n",
    "#     #     dataset.add_samples(samples_to_add)\n",
    "#     #     dataset.persistent = True # Save the dataset manifest\n",
    "#     #     print(f\"Created and saved FiftyOne dataset '{dataset.name}' with {len(dataset)} samples.\")\n",
    "\n",
    "#     # --- Launch the FiftyOne App ---\n",
    "#     # if dataset:\n",
    "#     #     session = fo.launch_app(dataset) \n",
    "#     #     print(f\"Launched FiftyOne app for dataset '{dataset.name}'. Check your browser.\")\n",
    "#     #     print(\"If the app doesn't open automatically, navigate to the URL shown in the console output from FiftyOne.\")\n",
    "#     #     # session.wait() # Uncomment to keep the notebook cell running until FiftyOne is closed by the user\n",
    "#     # else:\n",
    "#     #     print(\"No FiftyOne dataset was loaded or created. Skipping app launch.\")\n",
    "\n",
    "#     display(Markdown(\"**FiftyOne Integration Guide:**\"))\n",
    "#     print(\"The Python code in this cell is commented out by default to prevent errors if FiftyOne is not installed or data is not in the expected format.\")\n",
    "#     print(\"To use FiftyOne for interactive exploration:\")\n",
    "#     print(\"1. **Install FiftyOne**: `pip install fiftyone`\")\n",
    "#     print(\"2. **Prepare Data**: Ensure your image paths, ground truth labels, and model predictions are accessible.\")\n",
    "#     print(\"3. **Adapt Code**: Uncomment and modify the 'Option 2' section above to correctly load your data into a `fiftyone.Sample` list. You'll need to map your label format to `fiftyone.Detections` or `fiftyone.Classifications` as appropriate.\")\n",
    "#     print(\"4. **Launch**: Once the dataset is populated, `fo.launch_app(dataset)` will start the interactive UI.\")\n",
    "\n",
    "# except ImportError:\n",
    "#     display(Markdown(\"**FiftyOne is not installed.** Run `pip install fiftyone` to enable this feature for interactive dataset exploration and model evaluation.\"))\n",
    "# except Exception as e:\n",
    "#     display(Markdown(f\"**An error occurred related to FiftyOne:** `{e}`\"))\n",
    "#     print(\"Please ensure FiftyOne is correctly installed and your data loading logic for FiftyOne is accurate.\")\n",
    "\n",
    "print(\"This cell is a placeholder for FiftyOne integration.\")\n",
    "print(\"Refer to the commented-out code and Markdown instructions above for guidance on using FiftyOne.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c69338",
   "metadata": {},
   "source": [
    "Within the FiftyOne App, you can typically:\n",
    "*   **Visually inspect** images alongside their ground truth and predicted labels.\n",
    "*   **Filter and sort** samples based on various criteria (e.g., presence of certain labels, prediction confidence, correctness of predictions).\n",
    "*   Use the **Embeddings Panel** to visualize and explore high-dimensional image or object embeddings (if computed).\n",
    "*   **Tag samples** for further review or to create specific data slices.\n",
    "*   If an evaluation run has been added (e.g., `dataset.evaluate_detections(...)`), use the **Evaluation** tab to filter by *correct* or *incorrect* predictions and analyze confusion matrices interactively.\n",
    "\n",
    "This interactive exploration is invaluable for understanding nuanced model behavior, identifying challenging scenarios, discovering edge cases, and finding potential labeling errors in your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Summary, Conclusions & Recommendations\n",
    "\n",
    "This evaluation report has provided a detailed analysis of the **Photo‑Tag** multi‑label image classifier's performance on the validation set. Key quantitative metrics have been reported, and various visualizations offer qualitative insights into per-class behavior, error types, and overall model strengths and weaknesses.\n",
    "\n",
    "**Key Findings (Illustrative - replace with actual findings):**\n",
    "*   The model achieved a Macro-F1 score of [Value from metrics_results] and a mAP of [Value from metrics_results].\n",
    "*   Classes such as [List well-performing classes] showed strong performance, while [List poorly-performing classes] require further attention.\n",
    "*   Qualitative analysis revealed [Summarize any observed common error patterns or strengths].\n",
    "\n",
    "**Recommendations for Future Work:**\n",
    "\n",
    "1.  **Address Underperforming Classes**: For classes with low F1-scores (as identified in the per-class metrics and plots):\n",
    "    *   **Collect More Data / Augmentation**: Increase the number of samples for these classes, or apply targeted data augmentation techniques.\n",
    "    *   **Class-Balancing Strategies**: Experiment with class-balanced loss functions (e.g., Focal Loss, Class-Balanced Loss by Cui et al., LDAM Loss) or resampling techniques during training.\n",
    "2.  **Optimize Prediction Thresholds**: The default 0.5 threshold for converting probabilities to binary predictions may not be optimal across all classes.\n",
    "    *   Explore **per-class threshold optimization** on a dedicated validation set. Techniques include choosing thresholds that maximize F1-score per class or using criteria like Youden’s J statistic.\n",
    "3.  **Error Analysis & Hard Negative Mining**:\n",
    "    *   Utilize tools like **FiftyOne** for in-depth error analysis to understand why certain misclassifications occur.\n",
    "    *   If specific **hard-negative examples** (e.g., visually similar but incorrect classes being frequently confused) are identified, consider incorporating **hard-negative mining** into the training process.\n",
    "4.  **Model Architecture & Hyperparameters**:\n",
    "    *   Consider experimenting with different **backbone architectures** or **hyperparameter tuning** (e.g., learning rate, optimizer, regularization) if overall performance is not satisfactory.\n",
    "5.  **Iterate and Track Experiments**:\n",
    "    *   Rigorously **log all experiments**, including configurations, metrics, and generated artifacts (e.g., using MLflow, DVC, or similar MLOps tools). This is crucial for tracking progress, ensuring reproducibility, and making informed decisions in subsequent iterations of model development.\n",
    "\n",
    "This evaluation completes Step 4 of the pipeline. The generated artifacts in the `results/` folder provide a persistent record of this evaluation run, forming a baseline for future improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
