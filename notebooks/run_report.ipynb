{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Multi-Label Classification Validation Report\n",
    "\n",
    "This notebook automatically\n",
    "1. connects to MLflow;\n",
    "2. grabs the **latest** run in the experiment *MultilabelPhotoTagPipeline* (fallback = *Default* experiment);\n",
    "3. downloads metrics & artifacts; and\n",
    "4. visualises them ðŸ‘‰ precision / recall / F1 per label, confusion matrix, ROC curve (micro-average), loss curves.\n",
    "\n",
    "> **Tip for CI:** add a pipeline step `jupyter nbconvert --execute validation_report.ipynb` so the rendered notebook is saved as an artifact automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "import pandas as pd, numpy as np, json, os, matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "EXPERIMENT_NAME = \"MultilabelPhotoTagPipeline\"  # edit if needed\n",
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(EXPERIMENT_NAME) or client.get_experiment_by_name(\"Default\")\n",
    "runs = client.search_runs([exp.experiment_id], run_view_type=ViewType.ACTIVE_ONLY,\n",
    "                          order_by=[\"attributes.start_time DESC\"], max_results=1)\n",
    "if not runs:\n",
    "    raise RuntimeError(f\"No runs in experiment {exp.name}.\")\n",
    "run = runs[0]\n",
    "run_id = run.info.run_id\n",
    "print(\"Using run_id:\", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1  Classification metrics ----------\n",
    "report_path = client.download_artifacts(run_id, \"classification_report.json\", \".\")\n",
    "report = json.load(open(report_path))\n",
    "metrics_df = pd.DataFrame(report).T\n",
    "metrics_df.index.name = \"Label\"\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2  Confusion matrix heat-map ----------\n",
    "try:\n",
    "    cm_path = client.download_artifacts(run_id, \"confusion_matrix.png\", \".\")\n",
    "    img = plt.imread(cm_path)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Confusion matrix not logged:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3  ROC curve (micro-average) ----------\n",
    "try:\n",
    "    roc_path = client.download_artifacts(run_id, \"roc_curve.json\", \".\")\n",
    "    roc_data = json.load(open(roc_path))\n",
    "    fpr, tpr = np.array(roc_data[\"fpr\"]), np.array(roc_data[\"tpr\"])\n",
    "    auc_score = np.trapz(tpr, fpr)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"Micro ROC (AUC={auc_score:.2f})\")\n",
    "    plt.plot([0,1],[0,1],'k--', label=\"Chance\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (micro-average)\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"ROC curve not logged:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4  Loss curves ----------\n",
    "try:\n",
    "    hist_path = client.download_artifacts(run_id, \"history.json\", \".\")\n",
    "    history = json.load(open(hist_path))\n",
    "    epochs = range(1, len(history[\"loss\"]) + 1)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(epochs, history[\"loss\"], label=\"Train Loss\")\n",
    "    if \"val_loss\" in history:\n",
    "        plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"History not logged:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5  Micro / Macro / Weighted averages ----------\n",
    "for avg in (\"micro avg\", \"macro avg\", \"weighted avg\"):\n",
    "    if avg in metrics_df.index:\n",
    "        display(metrics_df.loc[[avg]])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
