{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1ce84b",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "This notebook is responsible for training the image classification model.\n",
    "It will load the processed data, define the model architecture, set up the training loop, and save the trained model and relevant artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "182d4491",
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root '/workspaces/photo_tag_pipeline' is already in sys.path.\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "from IPython.display import Image\n",
    "# Add the project root to the Python path\n",
    "# This allows importing modules from the 'src' directory\n",
    "current_path = Path(os.getcwd()).resolve()\n",
    "project_root = None\n",
    "# Iterate up from current_path to its parents\n",
    "for parent_dir in [current_path] + list(current_path.parents):\n",
    "    if (parent_dir / \".git\").is_dir() or (parent_dir / \"pyproject.toml\").is_file() or (parent_dir / \"src\").is_dir():\n",
    "        project_root = parent_dir\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    # Fallback for structures where notebook is in 'notebooks' dir directly under project root\n",
    "    if current_path.name == \"notebooks\" and (current_path.parent / \"src\").is_dir():\n",
    "        project_root = current_path.parent\n",
    "    else:\n",
    "        # Default to current_path if specific markers or 'notebooks' structure isn't found\n",
    "        project_root = current_path\n",
    "        print(f\"Warning: Could not reliably find project root. Using CWD: {project_root}. Ensure 'src' is in python path.\")\n",
    "\n",
    "if project_root:\n",
    "    project_root_str = str(project_root)\n",
    "    if project_root_str not in sys.path:\n",
    "        sys.path.insert(0, project_root_str)\n",
    "        print(f\"Project root '{project_root_str}' added to sys.path.\")\n",
    "    else:\n",
    "        print(f\"Project root '{project_root_str}' is already in sys.path.\")\n",
    "else:\n",
    "    print(\"Error: Project root could not be determined. Imports from 'src' may fail.\")\n",
    "\n",
    "# Reload modules to ensure the latest changes are picked up\n",
    "# Useful if you're actively developing the src modules\n",
    "import src.config\n",
    "import src.data.loader\n",
    "import src.models.PhotoTagNet_model\n",
    "import src.models.basic_model\n",
    "import src.utils.seed\n",
    "import src.utils.plot\n",
    "\n",
    "importlib.reload(src.config)\n",
    "importlib.reload(src.data.loader)\n",
    "importlib.reload(src.models.PhotoTagNet_model)\n",
    "importlib.reload(src.models.basic_model)\n",
    "importlib.reload(src.utils.seed)\n",
    "importlib.reload(src.utils.plot)\n",
    "from sympy import Basic\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "\n",
    "from src.models.PhotoTagNet_model import PhotoTagNet\n",
    "# Imports from our src directory\n",
    "from src.config import ModelConfig, OptimConfig, TrainConfig, CHECKPOINT_DIR, RESULTS_DIR\n",
    "from src.config import DEFAULT_CLASSES\n",
    "from src.data.loader import load_data\n",
    "from src.models.basic_model import BasicMLC\n",
    "from src.utils.seed import set_seed\n",
    "from src.utils.plot import save_loss_plot\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36be0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainConfig: TrainConfig(epochs=30, seed=42, precision_at_k=5, early_stop_patience=7)\n",
      "ModelConfig: ModelConfig(backbone='resnet50', pretrained=True, freeze_backbone=False, dropout_rate=0.7)\n",
      "OptimConfig: OptimConfig(optim='adamw', lr=0.0003, weight_decay=0.0001, betas=(0.9, 0.999), momentum=0.9, scheduler='step', step_size=5, gamma=0.5, patience=5)\n",
      "Seed set to 42\n",
      "Loading data...\n",
      "Data loaded. Train batches: 20, Val batches: 3\n",
      "Building model...\n",
      "Model, criterion, and optimizer created.\n",
      "Starting training for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Training]: 100%|██████████| 20/20 [00:22<00:00,  1.13s/batch, loss=0.34] \n",
      "Epoch 1/30 [Validation]: 100%|██████████| 3/3 [00:02<00:00,  1.32batch/s, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.4959, Val Loss: 0.3204\n",
      "New best model saved to /workspaces/photo_tag_pipeline/checkpoints/best_model_notebook.pth (Val Loss: 0.3204)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Training]: 100%|██████████| 20/20 [00:17<00:00,  1.15batch/s, loss=0.271]\n",
      "Epoch 2/30 [Validation]: 100%|██████████| 3/3 [00:02<00:00,  1.10batch/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Train Loss: 0.2252, Val Loss: 0.2455\n",
      "New best model saved to /workspaces/photo_tag_pipeline/checkpoints/best_model_notebook.pth (Val Loss: 0.2455)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Training]: 100%|██████████| 20/20 [00:14<00:00,  1.38batch/s, loss=0.251]\n",
      "Epoch 3/30 [Validation]: 100%|██████████| 3/3 [00:01<00:00,  2.57batch/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Train Loss: 0.1677, Val Loss: 0.2358\n",
      "New best model saved to /workspaces/photo_tag_pipeline/checkpoints/best_model_notebook.pth (Val Loss: 0.2358)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Training]: 100%|██████████| 20/20 [00:17<00:00,  1.13batch/s, loss=0.108] \n",
      "Epoch 4/30 [Validation]: 100%|██████████| 3/3 [00:01<00:00,  1.56batch/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Train Loss: 0.1211, Val Loss: 0.2548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Training]: 100%|██████████| 20/20 [00:17<00:00,  1.17batch/s, loss=0.0848]\n",
      "Epoch 5/30 [Validation]: 100%|██████████| 3/3 [00:02<00:00,  1.37batch/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Train Loss: 0.0886, Val Loss: 0.2678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Training]: 100%|██████████| 20/20 [00:17<00:00,  1.17batch/s, loss=0.0945]\n",
      "Epoch 6/30 [Validation]: 100%|██████████| 3/3 [00:02<00:00,  1.06batch/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Train Loss: 0.0677, Val Loss: 0.2681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Training]: 100%|██████████| 20/20 [00:19<00:00,  1.02batch/s, loss=0.104] \n",
      "Epoch 7/30 [Validation]: 100%|██████████| 3/3 [00:02<00:00,  1.00batch/s, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Train Loss: 0.0582, Val Loss: 0.3399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Training]: 100%|██████████| 20/20 [00:21<00:00,  1.06s/batch, loss=0.0456]\n",
      "Epoch 8/30 [Validation]: 100%|██████████| 3/3 [00:01<00:00,  1.74batch/s, loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Train Loss: 0.0456, Val Loss: 0.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Training]: 100%|██████████| 20/20 [00:24<00:00,  1.23s/batch, loss=0.0188]\n",
      "Epoch 9/30 [Validation]: 100%|██████████| 3/3 [00:01<00:00,  1.51batch/s, loss=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Train Loss: 0.0310, Val Loss: 0.3525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Training]:  15%|█▌        | 3/20 [00:05<00:29,  1.71s/batch, loss=0.034] "
     ]
    }
   ],
   "source": [
    "# Ensure results and plots directories exist for storing outputs\n",
    "PLOTS_DIR = RESULTS_DIR / \"plots\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True) \n",
    "PLOTS_DIR.mkdir(exist_ok=True, parents=True)   \n",
    "\n",
    "# ---- Configurations ----\n",
    "mcfg = ModelConfig()\n",
    "ocfg = OptimConfig()\n",
    "tcfg = TrainConfig() \n",
    "\n",
    "print(f\"TrainConfig: {tcfg}\")\n",
    "print(f\"ModelConfig: {mcfg}\")\n",
    "print(f\"OptimConfig: {ocfg}\")\n",
    "\n",
    "# ---- Set Seed ----\n",
    "set_seed(tcfg.seed)\n",
    "print(f\"Seed set to {tcfg.seed}\")\n",
    "\n",
    "# ---- Data Loaders ----\n",
    "print(\"Loading data...\")\n",
    "train_dataset, val_dataset, train_loader, val_loader = load_data()\n",
    "print(f\"Data loaded. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "\n",
    "# ---- Model, Loss, Optimizer ----\n",
    "print(\"Building model...\")\n",
    "model = BasicMLC(len(DEFAULT_CLASSES)).to(DEVICE)\n",
    "#model = PhotoTagNet(ModelConfig(), num_classes=len(DEFAULT_CLASSES)).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss() # Binary Cross-Entropy for multi-label with sigmoid output\n",
    "optimizer = optim.AdamW(model.parameters(), lr=ocfg.lr, weight_decay=ocfg.weight_decay)\n",
    "print(\"Model, criterion, and optimizer created.\")\n",
    "\n",
    "\n",
    "# ---- Training Loop ----\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "print(f\"Starting training for {tcfg.epochs} epochs...\")\n",
    "for epoch in range(tcfg.epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{tcfg.epochs} [Training]\", unit=\"batch\")\n",
    "    for imgs, labels in progress_bar:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        progress_bar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{tcfg.epochs} [Validation]\", unit=\"batch\")\n",
    "        for imgs, labels in progress_bar_val:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            progress_bar_val.set_postfix(loss=loss.item())\n",
    "            \n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{tcfg.epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # ---- Checkpoint ----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = CHECKPOINT_DIR / \"best_model_notebook.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved to {best_model_path} (Val Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "# ---- Save Final Model ----\n",
    "final_model_path = CHECKPOINT_DIR / \"final_model_notebook.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "# ---- Plot and Save Loss Curve ----\n",
    "# Ensure results directory exists (though config should handle it)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True) \n",
    "loss_plot_path = save_loss_plot(train_losses, val_losses, str(\"loss_curve_notebook.png\"))\n",
    "print(f\"Loss curve saved to {loss_plot_path}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Display the plot in the notebook\n",
    "display(Image(filename=str(loss_plot_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047feb24",
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow UI started on port 5000.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Check if mlflow is installed and start the UI if available\n",
    "try:\n",
    "    mlflow_port = 5000\n",
    "    mlflow_ui_proc = subprocess.Popen(\n",
    "        [\"mlflow\", \"ui\", \"--port\", str(mlflow_port), \"--host\", \"0.0.0.0\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    print(f\"MLflow UI started on port {mlflow_port}.\")\n",
    "    # Open in host browser if $BROWSER is available\n",
    "    if \"BROWSER\" in os.environ:\n",
    "        os.system(f'$BROWSER http://localhost:{mlflow_port}')\n",
    "    else:\n",
    "        print(f\"Open http://localhost:{mlflow_port} in your browser.\")\n",
    "except ImportError:\n",
    "    print(\"mlflow is not installed. Please install it with `pip install mlflow`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b133aa",
   "metadata": {},
   "source": [
    "After training, the model will be saved to the `checkpoints/` directory (e.g., `best_model_notebook.pth`, `final_model_notebook.pth`), and the loss curve plot will be saved in the `results/` directory (e.g., `loss_curve_notebook.png`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024e8dc",
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 03_train_model.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 347415 bytes to ../assets/03_train_model_export.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=padding:10px;'><h3>Export complete!</h3><p>Notebook <b>03_train_model.ipynb</b> has been exported to HTML.</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import HTML\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Get current notebook name\n",
    "try:\n",
    "    # Try to get the notebook name using IPython's special variable\n",
    "    notebook_path = get_ipython().kernel.shell.user_ns['__vsc_ipynb_file__'] if '__vsc_ipynb_file__' in get_ipython().kernel.shell.user_ns else None\n",
    "    if not notebook_path:\n",
    "        notebook_path = get_ipython().kernel.shell.user_ns.get('__notebook_source__', '')\n",
    "    notebook_name = os.path.basename(notebook_path) if notebook_path else ''\n",
    "except:\n",
    "    notebook_name = ''\n",
    "\n",
    "# Check if the notebook name was successfully determined\n",
    "if notebook_name == '':\n",
    "    # Fallback method if automatic detection fails\n",
    "    notebook_name = input(\"Enter notebook filename (with .ipynb extension): \")\n",
    "\n",
    "# Use nbconvert to export the notebook without input cells tagged with \"remove\"\n",
    "# Ensure the assets directory exists\n",
    "assets_dir = \"../assets\"\n",
    "os.makedirs(assets_dir, exist_ok=True)\n",
    "output_html = os.path.join(assets_dir, os.path.splitext(os.path.basename(notebook_name))[0] + \"_export.html\")\n",
    "!jupyter nbconvert --to html --TagRemovePreprocessor.remove_cell_tags='{\"remove\"}' \"{notebook_name}\" --output \"{output_html}\"\n",
    "\n",
    "# Display a success message\n",
    "display(HTML(f\"<div style=padding:10px;'>\"\n",
    "             f\"<h3>Export complete!</h3>\"\n",
    "             f\"<p>Notebook <b>{notebook_name}</b> has been exported to HTML.</p>\"\n",
    "             f\"</div>\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
