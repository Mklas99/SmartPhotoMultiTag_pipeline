# Multi-Label Photo Tagger - Evaluation Pipeline

This directory contains an integrated evaluation pipeline for the Multi-Label Photo Tagger project. The pipeline consists of two main components that work together to provide comprehensive model evaluation and visualization.

## üîó Pipeline Components

### 1. `scripts/evaluate_last_run.py` - Evaluation Engine
**Purpose:** Computes evaluation metrics and generates artifacts for the latest MLflow run.

**What it does:**
- Downloads prediction arrays from MLflow run artifacts
- Computes classification metrics (precision, recall, F1-score, accuracy)
- Generates confusion matrices (overall + per-class)
- Creates ROC curve data and calculates AUC scores
- Saves all results as MLflow artifacts for visualization

**Usage:**
```bash
# Evaluate the latest run in the default experiment
python scripts/evaluate_last_run.py

# Specify a different experiment
python scripts/evaluate_last_run.py --experiment MyExperiment

# Evaluate a specific run ID
python scripts/evaluate_last_run.py --run-id abc123456789
```

**Generated Artifacts:**
- `classification_report.json` - Detailed per-class metrics
- `confusion_matrix.png` - Combined confusion matrix visualization
- `confusion_matrix_{class}.png` - Individual class confusion matrices
- `roc_curve.json` - ROC curve data and AUC score
- `evaluation_summary.json` - Complete evaluation metadata

### 2. `notebooks/run_report.ipynb` - Visualization Dashboard
**Purpose:** Creates beautiful, interactive visualizations from evaluation artifacts.

**What it displays:**
- üéØ Per-label performance metrics with color-coded cards
- üî• Confusion matrix heatmaps and accuracy analysis
- üìà ROC curves with performance interpretation
- üìä Training progress and loss analysis from MLflow metrics
- üéØ Comprehensive performance dashboard with recommendations
- üì§ HTML export functionality for sharing

**Dependencies:** Requires `evaluate_last_run.py` to be executed first.

## üöÄ Complete Workflow

### Step 1: Train Your Model
Run your training script to create an MLflow run with predictions:
```bash
python scripts/run_train.py  # or your training script
```

### Step 2: Generate Evaluation Artifacts
Execute the evaluation script to compute metrics and create visualizations:
```bash
python scripts/evaluate_last_run.py
```

### Step 3: Visualize Results
Open and run the Jupyter notebook to see interactive visualizations:
```bash
jupyter notebook notebooks/run_report.ipynb
```

### Step 4: Export Report (Optional)
The notebook can export a complete HTML report for sharing with your team.

## üìã Data Flow Diagram

```
Training Script
    ‚Üì (logs predictions)
MLflow Run
    ‚Üì (downloads artifacts)
evaluate_last_run.py
    ‚Üì (generates evaluation artifacts)
MLflow Artifacts
    ‚Üì (reads artifacts)
run_report.ipynb
    ‚Üì (creates visualizations)
Interactive Dashboard + HTML Export
```

## üõ†Ô∏è Technical Requirements

### For `evaluate_last_run.py`:
- MLflow run with `y_val.npy` and `y_pred_prob.npy` artifacts
- Run parameter `classes` containing class names
- Python packages: `mlflow`, `scikit-learn`, `matplotlib`, `seaborn`, `numpy`

### For `run_report.ipynb`:
- Evaluation artifacts generated by `evaluate_last_run.py`
- MLflow training metrics (train_loss, val_loss) for progress analysis
- Python packages: `mlflow`, `pandas`, `matplotlib`, `seaborn`, `jupyter`

## üéØ Key Features

### Integration Benefits:
- **Separation of Concerns**: Computation (script) vs. Visualization (notebook)
- **Reproducible**: All artifacts stored in MLflow for consistent results
- **Flexible**: Can re-run visualization without re-computing metrics
- **Shareable**: HTML export for documentation and team collaboration
- **Scalable**: Works with any number of classes and experiments

### Error Handling:
- Clear error messages when artifacts are missing
- Graceful degradation when optional components aren't available
- Helpful instructions for resolving common issues

## üîß Troubleshooting

### "No evaluation artifacts found"
**Solution:** Run `python scripts/evaluate_last_run.py` first.

### "Classification Report Not Available"
**Cause:** Missing `y_val.npy` or `y_pred_prob.npy` artifacts in MLflow run.
**Solution:** Ensure your training script saves these arrays as MLflow artifacts.

### "Training History Not Available"
**Cause:** No `train_loss` or `val_loss` metrics logged to MLflow.
**Solution:** Add MLflow metric logging to your training loop:
```python
mlflow.log_metric("train_loss", loss_value, step=epoch)
mlflow.log_metric("val_loss", val_loss_value, step=epoch)
```

## üìö Example Output

The pipeline generates:
- **Performance Cards**: Visual metrics for each class with color coding
- **Confusion Matrices**: Both combined and per-class visualizations
- **ROC Analysis**: Curves with AUC interpretation and performance rating
- **Training Curves**: Loss progression and overfitting analysis
- **Recommendations**: Actionable insights for model improvement
- **HTML Reports**: Professional documentation for sharing

---

This integrated pipeline ensures that your model evaluation is both thorough and visually compelling, making it easy to understand model performance and share results with stakeholders.
